{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e551d21-2d57-4e7d-ab6a-dd950af7ee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fourInRowGame import Chip, FourInRowGame\n",
    "import models\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import sys\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa147394-2b4c-46ed-b341-843617cadc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a game\n",
    "nrow = 6\n",
    "ncol = 7\n",
    "num_states = [nrow, ncol]\n",
    "env = FourInRowGame(nrow, ncol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3813dc2-d31c-4f41-9d2a-d98728f37a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploration_policy(eps):\n",
    "    act = np.random.choice(['model','random'], p = [1 - eps, eps])\n",
    "    return act\n",
    "\n",
    "\n",
    "def get_action(policy_net, state, eps):\n",
    "    #Get the action based on greedy epsilon policy\n",
    "    act = exploration_policy(eps)\n",
    "    #Get predictions\n",
    "    preds = policy_net(state)\n",
    "    weights = preds.clone().numpy(force=True)\n",
    "    if act == 'model':\n",
    "        action = np.argmax(weights)\n",
    "    elif act == 'random':\n",
    "        action = np.random.randint(0, 7)\n",
    "    else:\n",
    "        raise ValueError(f\"act is {act}. Manno\")\n",
    "    return int(action), weights\n",
    "\n",
    "\n",
    "def check_valid(action, weights, env):\n",
    "    cont = True\n",
    "    while cont:    \n",
    "        if env.column_height(action) >= 6:\n",
    "            if np.isfinite(weights).sum() != 0:\n",
    "                weights[action] = np.NINF\n",
    "                action = np.argmax(weights)\n",
    "            else:\n",
    "                raise ValueError(f\"Someone managed to violate the rules. I can not move and therefore the game has already ended!\")\n",
    "        else:\n",
    "            cont = False    \n",
    "    return int(action)\n",
    "\n",
    "\n",
    "def get_reward(winner, terminated):\n",
    "    if not terminated:\n",
    "        reward = 0.1\n",
    "    else:\n",
    "        if winner == 1:  # Model won\n",
    "            reward = 50.\n",
    "        elif winner == -1:  # Model lost\n",
    "            reward = -100.\n",
    "        else:  # Model drew\n",
    "            reward = 10.\n",
    "    return reward\n",
    "\n",
    "\n",
    "def make_step(env, action, chip1, chip2, policy_net, mode=\"self\"):\n",
    "    terminated = False\n",
    "    # Model is player 1 and makes a step\n",
    "    env.drop(chip1, action)\n",
    "    if env.check_for_victory():\n",
    "        winner = 1\n",
    "        terminated = True\n",
    "    else:\n",
    "        not_full_cols = 0\n",
    "        for col in range(6):\n",
    "           if env.column_height(col) <= 5:\n",
    "               not_full_cols += 1\n",
    "        if not_full_cols == 0:\n",
    "            terminated = True\n",
    "        winner = 0\n",
    "    if not terminated:\n",
    "        if \"trans\" in mode: \n",
    "            mode = np.random.choice(['self','rng'], p = [1 - trans_eps, trans_eps])\n",
    "        if \"self\" in mode:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(env.get_simple_slots(), dtype=torch.float)\n",
    "                preds = policy_net(state)\n",
    "                weights = preds.clone().numpy(force=True)\n",
    "                action = int(np.argmax(weights))\n",
    "                action = check_valid(action, weights, env)\n",
    "                env.drop(chip2, action)\n",
    "        elif \"rng\" in mode:\n",
    "            # Opponent is a random player and makes step\n",
    "            pos_acts = []\n",
    "            for action in range(0,6):\n",
    "                if env.column_height(action) <= 5:\n",
    "                    pos_acts.append(action)\n",
    "            pos_acts = np.array(pos_acts)\n",
    "            if pos_acts.size != 0:\n",
    "                action2 = np.random.choice(pos_acts)\n",
    "                env.drop(chip2, action2)\n",
    "            else:\n",
    "                raise ValueError(\"Choices Empty, your draw check sucks.\")\n",
    "        if env.check_for_victory():\n",
    "            winner = -1\n",
    "            terminated = True\n",
    "        else:\n",
    "            no_full_cols = 0\n",
    "            for col in range(6):\n",
    "               if env.column_height(col) <= 5:\n",
    "                   no_full_cols += 1\n",
    "            if no_full_cols == 0:\n",
    "                terminated = True\n",
    "            winner = 0\n",
    "    return env, winner, terminated\n",
    "\n",
    "\n",
    "def optimize_model(optimizer, states, acts, rewards, gamma, policy_net, target_net):\n",
    "    size = len(states)\n",
    "    poli_preds = policy_net(states)\n",
    "    if poli_preds.ndim >= 3:\n",
    "        poli_preds = poli_preds.squeeze()\n",
    "    state_action_values = poli_preds.gather(1, acts.type(torch.int64))\n",
    "    next_state_values = torch.zeros(size)\n",
    "    with torch.no_grad():\n",
    "        tar_preds = target_net(states[1:])\n",
    "        if tar_preds.ndim >= 3:\n",
    "            tar_preds = tar_preds.squeeze()\n",
    "        next_state_values[:-1] = tar_preds.max(1).values\n",
    "    expected_state_action_values = (next_state_values * gamma) + rewards\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    # print(f\"state_action_values is {state_action_values}\\n\")\n",
    "    # print(f\"expected_state_action_values is {expected_state_action_values}\\n\")\n",
    "    # print(f\"in unsqueezed: {expected_state_action_values.unsqueeze(1)}\")\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7678b4bf-78a4-412e-9542-a03386b107b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(env, optimizer, policy_net, target_net, learn_rate, eps, eps_rate, eps_min, mode, tau, gamma, chip_model, chip_opponent, path=\"model_name\"):\n",
    "    counter = 0\n",
    "    encounters = 100000\n",
    "    pbar = tqdm.tqdm(range(encounters))\n",
    "    for episode in pbar:\n",
    "        # Reset the environment to an empty board\n",
    "        env.reset()\n",
    "        states = torch.ones((1,6,7))\n",
    "        acts = torch.ones((1,1))\n",
    "        rewards = torch.ones((1))\n",
    "        eps = eps * eps_rate\n",
    "        if eps <= eps_min:\n",
    "            eps = eps_min\n",
    "        terminated = False\n",
    "        model_is_first = np.random.randint(2)\n",
    "        if model_is_first == 0:\n",
    "            action = np.random.randint(0, 7)\n",
    "            env.drop(chip_opponent, action)\n",
    "        while not terminated:\n",
    "            states = torch.cat((states,(torch.tensor(env.get_simple_slots(), dtype=torch.float)).unsqueeze(0)))\n",
    "            action, weights = get_action(policy_net, states[-1], eps)\n",
    "            action = check_valid(action, weights, env)\n",
    "            env, winner, terminated = make_step(env, action, chip_model, chip_opponent, policy_net, mode)\n",
    "            reward = get_reward(winner, terminated)\n",
    "            # action_ohe = torch.zeros((1,1))\n",
    "            # action_ohe[:,action] = 1\n",
    "            acts = torch.cat((acts, torch.tensor([[action]])))\n",
    "            rewards = torch.cat((rewards, torch.tensor([reward])))\n",
    "            if winner == 1:\n",
    "                counter += 1\n",
    "        loss = optimize_model(optimizer, states[1:], acts[1:], rewards[1:], gamma, policy_net, target_net)\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*tau + target_net_state_dict[key]*(1-tau)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "        pbar.set_description(f\"Loss: {loss}\")\n",
    "    torch.save(policy_net.state_dict(), path)\n",
    "    print(f\"The model won {counter/encounters}% of its encounters\")\n",
    "    return policy_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba6486aa-1d6e-4809-9076-de9063a7291e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 547243828445184.0: 100%|█████████████████████████████████████████████████| 100000/100000 [49:17<00:00, 33.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model won 0.53051% of its encounters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.99\n",
    "eps = 1\n",
    "eps_rate = 0.995\n",
    "eps_min = 0.1\n",
    "learn_rate = 0.001\n",
    "tau = 0.005\n",
    "mode = \"self\"\n",
    "chip_model = Chip.RED\n",
    "chip_opponent = Chip.YELLOW\n",
    "model_name = \"FCmedium_1402_diff_ret\"\n",
    "policy_net = models.FCmedium(env.columns*env.rows, env.columns)\n",
    "target_net = models.FCmedium(env.columns*env.rows, env.columns)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=learn_rate)\n",
    "#for i in range(11):\n",
    "#    path = f\"{model_name}/Iteration_{i}\"\n",
    "#    if not os.path.exists(model_name):\n",
    "#        os.makedirs(model_name)\n",
    "#    path_pre = f\"{model_name}/Iteration_{i-1}\"\n",
    "#    if os.path.exists(path_pre):\n",
    "#        target_net = models.CNNmedium(1, env.columns)\n",
    "#        target_net.load_state_dict(torch.load(path_pre))\n",
    "target_net = main(env, \n",
    "                      optimizer, \n",
    "                      policy_net, \n",
    "                      target_net, \n",
    "                      learn_rate, \n",
    "                      eps, \n",
    "                      eps_rate, \n",
    "                      eps_min, \n",
    "                      mode, \n",
    "                      tau, \n",
    "                      gamma, \n",
    "                      chip_model, \n",
    "                      chip_opponent, \n",
    "                      path = model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
