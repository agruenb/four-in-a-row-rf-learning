{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fced1ae9-68eb-499f-bc73-858541ca1361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fourInRowGame import Chip, FourInRowGame\n",
    "import models\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import sys\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88342434-5110-4527-99a8-62e95d4b341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a game\n",
    "nrow = 6\n",
    "ncol = 7\n",
    "num_states = [nrow, ncol]\n",
    "env = FourInRowGame(nrow, ncol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1190b088-606e-42da-9e49-897740572f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploration_policy(eps):\n",
    "    act = np.random.choice(['model','random'], p = [1 - eps, eps])\n",
    "    return act\n",
    "\n",
    "\n",
    "def get_action(model, state, eps):\n",
    "    #Get the action based on greedy epsilon policy\n",
    "    act = exploration_policy(eps)\n",
    "    #Get predictions\n",
    "    preds = model(state)\n",
    "    weights = preds.clone().numpy(force=True)\n",
    "    if act == 'model':\n",
    "        action = np.argmax(weights)\n",
    "    elif act == 'random':\n",
    "        action = np.random.randint(0, 7)\n",
    "    else:\n",
    "        raise ValueError(f\"act is {act}. Manno\")\n",
    "    return int(action), weights\n",
    "\n",
    "\n",
    "def check_valid(action, weights, env):\n",
    "    cont = True\n",
    "    while cont:    \n",
    "        if env.column_height(action) >= 6:\n",
    "            if np.isfinite(weights).sum() != 0:\n",
    "                weights[action] = np.NINF\n",
    "                action = np.argmax(weights)\n",
    "            else:\n",
    "                raise ValueError(f\"Someone managed to violate the rules. I can not move and therefore the game has already ended!\")\n",
    "        else:\n",
    "            cont = False    \n",
    "    return action\n",
    "\n",
    "\n",
    "def get_reward(winner, terminated):\n",
    "    if not terminated:\n",
    "        reward = 0.1\n",
    "    else:\n",
    "        if winner == 1:  # Model won\n",
    "            reward = 0.\n",
    "        elif winner == -1:  # Model lost\n",
    "            reward = 1000.\n",
    "        else:  # Model drew\n",
    "            reward = 10.\n",
    "    return reward\n",
    "\n",
    "\n",
    "def make_step(env, action, chip1, chip2, model, mode=\"self\", trans_eps = 0):\n",
    "    terminated = False\n",
    "    # Model is player 1 and makes a step\n",
    "    env.drop(chip1, action)\n",
    "    if env.check_for_victory():\n",
    "        winner = 1\n",
    "        terminated = True\n",
    "    else:\n",
    "        not_full_cols = 0\n",
    "        for col in range(6):\n",
    "           if env.column_height(col) <= 5:\n",
    "               not_full_cols += 1\n",
    "        if not_full_cols == 0:\n",
    "            terminated = True\n",
    "        winner = 0\n",
    "    if not terminated:\n",
    "        if \"trans\" in mode: \n",
    "            mode = np.random.choice(['self','rng'], p = [1 - trans_eps, trans_eps])\n",
    "        if \"self\" in mode:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(env.get_simple_slots(), dtype=torch.float)\n",
    "                preds = model(state)\n",
    "                weights = preds.clone().numpy(force=True)\n",
    "                action = int(np.argmax(weights))\n",
    "                action = check_valid(action, weights, env)\n",
    "                env.drop(chip2, action)\n",
    "        elif \"rng\" in mode:\n",
    "            # Opponent is a random player and makes step\n",
    "            pos_acts = []\n",
    "            for action in range(0,6):\n",
    "                if env.column_height(action) <= 5:\n",
    "                    pos_acts.append(action)\n",
    "            pos_acts = np.array(pos_acts)\n",
    "            if pos_acts.size != 0:\n",
    "                action2 = np.random.choice(pos_acts)\n",
    "                env.drop(chip2, action2)\n",
    "            else:\n",
    "                raise ValueError(\"Choices Empty, your draw check sucks.\")\n",
    "        if env.check_for_victory():\n",
    "            winner = -1\n",
    "            terminated = True\n",
    "        else:\n",
    "            no_full_cols = 0\n",
    "            for col in range(6):\n",
    "               if env.column_height(col) <= 5:\n",
    "                   no_full_cols += 1\n",
    "            if no_full_cols == 0:\n",
    "                terminated = True\n",
    "            winner = 0\n",
    "    return env, winner, terminated\n",
    "\n",
    "\n",
    "\n",
    "def train_step(model, criterion, optimizer, states, actions, rewards):    \n",
    "    preds = model(states)\n",
    "    loss = criterion(preds, actions) * np.mean(-rewards)\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90d9138b-8749-4734-b541-e1f876b05364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(env, model, learn_rate, criterion, eps, eps_rate, eps_min, mode, chip_model, chip_opponent, path=\"model_name\"):\n",
    "    # Set optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "    counter = 0\n",
    "    encounters = 10000\n",
    "    trans_eps = 1\n",
    "    pbar = tqdm.tqdm(range(encounters))\n",
    "    for episode in pbar:\n",
    "        # Reset the environment to an empty board\n",
    "        env.reset()\n",
    "        states = torch.ones((1,6,7))\n",
    "        acts = torch.ones((1,7))\n",
    "        rewards = []\n",
    "        eps = eps * eps_rate\n",
    "        trans_eps = trans_eps * 0.999\n",
    "        if eps <= eps_min:\n",
    "            eps = eps_min\n",
    "        terminated = False\n",
    "        model_is_first = np.random.randint(2)\n",
    "        if model_is_first == 0:\n",
    "            action = np.random.randint(0, 7)\n",
    "            env.drop(chip_opponent, action)\n",
    "        while not terminated:\n",
    "            states = torch.cat((states,(torch.tensor(env.get_simple_slots(), dtype=torch.float)).unsqueeze(0)))\n",
    "            action, weights = get_action(model, states[-1], eps)\n",
    "            action = check_valid(action, weights, env)\n",
    "            env, winner, terminated = make_step(env, action, chip_model, chip_opponent, model, mode, trans_eps)\n",
    "            reward = get_reward(winner, terminated)\n",
    "            action_ohe = torch.full((1,7), 0.05)\n",
    "            action_ohe[:,action] = 0.7\n",
    "            acts = torch.cat((acts, action_ohe))\n",
    "            rewards.append(reward)\n",
    "            if winner == 1:\n",
    "                counter += 1\n",
    "        preds = model(states[1:])\n",
    "        if preds.ndim >= 3:\n",
    "            preds = preds.squeeze()\n",
    "        loss = criterion(preds, acts[1:]) * np.mean(rewards)\n",
    "        optimizer.step()\n",
    "        pbar.set_description(f\"Loss: {loss}\")\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"The model won {counter/encounters}% of its encounters\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cef5176c-2374-4e22-bff0-ba400acc254b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.18043816089630127: 100%|█████████████████████████████████████████████████| 10000/10000 [04:13<00:00, 39.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model won 0.5049% of its encounters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model_structure = models.FCmedium\n",
    "env = FourInRowGame(6, 7)\n",
    "learn_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "eps = 1\n",
    "eps_rate = 0.995\n",
    "eps_min = 0.1\n",
    "mode = \"trans\"\n",
    "chip_model = Chip.RED\n",
    "chip_opponent = Chip.YELLOW\n",
    "model = models.CNNmedium(1, env.columns)\n",
    "model_name = \"CNNmedium\"\n",
    "for i in range(1):\n",
    "    path = f\"{model_name}/Iteration_{i}\"\n",
    "    if not os.path.exists(model_name):\n",
    "        os.makedirs(model_name)\n",
    "    path_pre = f\"{model_name}/Iteration_{i-1}\"\n",
    "    if os.path.exists(path_pre):\n",
    "        model = models.CNNmedium(1, env.columns)\n",
    "        model.load_state_dict(torch.load(path_pre))\n",
    "    model = main(env, model, learn_rate, criterion, eps, eps_rate, eps_min, mode, chip_model, chip_opponent, path=path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
